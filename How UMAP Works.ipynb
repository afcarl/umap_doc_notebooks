{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How UMAP Works\n",
    "\n",
    "UMAP is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. It provides a very general framework for approaching manifold learning and dimension reduction, but can also provide specific concrete realisations. This article will discuss how the algorithm works in practice. There exist deeper mathematical underpinnings, but for the sake of readability by a general audience these will merely be referenced and linked. If you are looking for the mathematical description please see the [UMAP paper](https://arxiv.org/abs/1803.xxxx).\n",
    "\n",
    "To begin making sense of UMAP we will need a little bit of mathematical background from algebraic topology and topological data analysis. This will provide a basic algorithm that works well in theory, but unfortunately not so well in practice. The next step will be to make use of some basic Riemannian geometry to bring real world data a little closer to the underlying assumptions of the topological data analysis algorithm. Unfortunately this will introduce new complications, which will be resolved through a combination of deep math (details of which will be elided) and fuzzy logic. We can then put the pieces back together again, and combine them with a new approach to finding a low dimensional representation more fitting to the new data structures at hand. Putting this all together we arrive at the basic UMAP algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topological Data Analysis and Simplicial Complexes\n",
    "\n",
    "Simplicial complexes are a means to construct topological spaces out of simple combinatorial components. This allows one to reduce to complexities of dealing with the continuous geometry of topological spaces to the task of relatively simple combinatorics and counting. This method of taming geometry and topology will be fundamental to our approach to topological data analysis in general, and dimension reduction in particular.\n",
    "\n",
    "The first step is to provide some simple combinatorial building blocks called [*simplices*](https://en.wikipedia.org/wiki/Simplex). Geometrically a simplex is a very simple way to build an $k$-dimensional object. A $k$ dimensional simplex is called a $k$-simplex, and it is formed by taking the convex hull of $k+1$ independent points. Thus a 0-simplex is a point, a 1-simplex is a line segment (between two zero simplices), a 2-simplex is a triangle (with three 1-simplices as \"faces\"), and a 3-simplex is a tetrahedron (with four 2-simplices as \"faces\"). Such a simple construction allows for easy generalisation to arbitrary dimensions.\n",
    "\n",
    "![Low dimensional simplices](simplices.png)\n",
    "\n",
    "This has a very simple combinatorial underlying structure, and ultimately one can regard a $k$-simplex as an arbitrary set of $k+1$ objects with faces (and faces of faces etc.) given by appropriately sized subsets -- one can always provide a \"geometric realization\" of this abstract set description by constructing the corresponding geometric simplex.\n",
    "\n",
    "Simplices can provide building blocks, but to construct interesting topological spaces we need to be able to glue together such building blocks. This can be done by constructing a [*simplicial complex*](https://en.wikipedia.org/wiki/Simplicial_complex). Ostensibly a simplicial complex is a set of simplices glued together along faces. More explicitly a simplicial complex $\\mathcal{K}$ is a set of simplices such that any face of any simplex in $\\mathcal{K}$ is also in $\\mathcal{K}$ (ensuring all faces exist), and the intersection of any two simplices in $\\mathcal{K}$ is a face of both simplices. A large class of topological spaces can be constructed in this way -- just gluing together simplices of various dimensions along their faces. A little further abstraction will get to [*simplicial sets*](https://en.wikipedia.org/wiki/Simplicial_set) which are purely combinatorial, have a nice category theoretic presentation, and can generate a much broader class of topological spaces, but that will take us to far afield for this article. The intuition of simplicial complexes will be enough to illustrate the relevant ideas and motivation.\n",
    "\n",
    "How does apply these theoretical tools from topology to finite sets of data points? To start we'll look at how one might construct a simplicial complex from topological space. The tool we will consider is the construction of a [Čech complex](https://en.wikipedia.org/wiki/%C4%8Cech_cohomology) given an <a href=\"https://en.wikipedia.org/wiki/Cover_(topology)#open_cover\">open cover</a> of a topological space. That's a lot of verbiage if you haven't done much topology, but we can break it down fairly easily for our use case. An open cover is essentially just a family sets whose union is the whole space, and a Čech complex is a combinatorial way to covert that into a simplicial complex. It works fairly simply: let each set in the cover be a 0-simplex; create a 1-simplex between two such sets if they have a non-empty intersection; create a 2-simplex between three such sets if the triple intersection of all three is non-empty; and so on. Now, that doesn't sound very advanced -- just looking at intersections of sets. The key is that the background topological theory actually provides guarantees about how well this simple process can produce something that represents the topological space itself in a meaningful way. Obviously the quality of the cover is important, and finer covers provide more accuracy, but the reality is that despite its simplicity the process captures much of the topology.\n",
    "\n",
    "Next we need to understand how to apply that process to a finite set of data samples. If we assume that the data samples are drawn from some underlying topological space then to learn about the topology of that space we need to generate an open cover of it. If our data actually lies in a metric space (i.e. we can measure distance between points) then one way to approximate an open cover is to simply create balls of some fixed radius about each data point. Since we only have finite samples, and not the topological space itself, we cannot be sure it is truly an open cover, but it is might be as good an approximation as we could reasonably expect. This approach also has the advantage that the Čech complex associated to the cover will have a 0-simplex for each data point.\n",
    "\n",
    "To demonstrate the process let's consider a test dataset like this\n",
    "\n",
    "![Test data set of a noisy sine wave](how_umap_works_raw_data.png)\n",
    "\n",
    "If we fix a radius we can then picture the open sets of our cover as circles (since we are in a nice visualizable two dimensional case). The result is something like this\n",
    "\n",
    "![A basic open cover of the test data](how_umap_works_open_cover.png)\n",
    "\n",
    "We can then depict the the simplcial complex of 0-, 1-, and 2-simplices as points, lines, and triangles\n",
    "\n",
    "![A simplicial complex built from the test data](how_umap_works_basic_graph.png)\n",
    "\n",
    "It is harder to easily depict the higher dimensional simplices, but you can imagine how they would fit in. There are two things to note here: first, the simplicial complex does a reasonable job of starting to capture the fundamental topology of the dataset; second, most of the work is really done by the 0- and 1-simplices, which are easier to deal with computationally (it is just a graph, in the nodes and edges sense). The second observation motivates the [Vietoris-Rips complex](https://en.wikipedia.org/wiki/Vietoris%E2%80%93Rips_complex), which is similar to the Čech complex but is entirely determined by the 0- and 1-simplices. Vietoris-Rips complexes are much easier to work with computationally, especially for large datasets, and are one of the major tools of topological data analysis.\n",
    "\n",
    "If we take this approach to get a topological representation then we can build a dimension reduction algorithm by finding a low dimensional representation of the data that has a similar topological representation. If we only care about the 0- and 1-simplices then the topological representation is just a graph, and finding a low dimensional representation can be described as a [graph layout problem](). If one wants to use, for example, spectral methods for graph layout then we arrive at algorithms like [Laplacian eigenmaps]() and [Diffusion maps](). Force directed layouts are also an option, and provide algorithms closer to [MDS]() or [Sammon mapping]() in flavour.\n",
    "\n",
    "I would not blame those who have read this far to wonder why we took such an abstract roundabout road to simply building a neighborhood-graph on the data and then laying out that graph. There are a couple of reasons. The first reason is that the topological approach, while abstract, provides sound theoretical justification for what we are doing. While building a neighborhood-graph and laying it out in lower dimensional space make heuristic sense and is computationally tractable, it doesn't provide the same underlying motivation of capturing the underlying topological structure of the data faithfully -- for that we need to appeal to the powerful topological machinery I've hinted lies in the background. The second reason is that it is this more abstract topological approach that will allow us to generalise the approach and get around some of the difficulties of the sorts of algorithms described above. While ultimately we will end up with a process that is fairly simple computationally, understanding *why* various manipulations matter is important to truly understanding the algorithm (as opposed to merely computing with it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to Real World Data\n",
    "\n",
    "The approach described above provides a nice theory for why a neighborhood graph based approach should capture manifold structure when doing dimension reduction. The problem tends to come when one tries to put the theory into practice. The first obvious difficulty (and we can see it even our example above) is that choosing the right radius for the balls that make up the open cover is hard. If you choose something too small the resulting simplicial complex splits nto many connected components. If you choose something too large the simplicial complex turns into just a few very high dimensional simplices (and their faces etc.) and fails to capture the manifold structure anymore. How should one solve this?\n",
    "\n",
    "A standard approach for algorithms that take a purely graph based approach is to use a *k*-neighbor graph instead of using balls of some fixed radius to define connectivity. What this means is that each point in the dataset is given an edge to each of its *k* nearest neighbors. Once can then try to embed the resulting graph. This has the advantage that we no longer have to choose the radius of the balls, but it does so by introducing a new parameter *k*. This is still a good trade -- it is often easier to pick a resolution scale in terms of number of neighbors than it is to correctly choose a distance. This is because choosing a distance is very dataset dependent: one needs to look at the distribution of distances in the dataset to even begin to select a good value. In contrast, while a *k* value is still dataset dependent to some degree, there are reasonable default choices, such as the 10 nearest neighbors, that should work acceptably for most datasets.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Low Dimensional Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UMAP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:umap_dev]",
   "language": "python",
   "name": "conda-env-umap_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
